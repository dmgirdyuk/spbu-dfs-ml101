{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Авторы: Анастасия Никольская, Антон Першин, Гирдюк Дмитрий\n",
    "\n",
    "Датасет скачивать по ссылке: https://disk.yandex.ru/d/cwL3Ka4ECyQwpw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общая информация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(cfg[\"house_prices\"][\"train_dataset\"])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не все столбцы здесь выведены. Их список мы можем получить, используя аттрибут `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почистим данные в нескольких столбцах, основываясь на 'data_description.txt' датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Exterior2nd\"] = df_train[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n",
    "\n",
    "# some values of GarageYrBlt are corrupt, so we'll replace them with the year the house was built\n",
    "df_train[\"GarageYrBlt\"] = df_train[\"GarageYrBlt\"].where(df_train.GarageYrBlt <= 2010, df_train.YearBuilt)\n",
    "\n",
    "# names beginning with numbers are awkward to work with\n",
    "df_train.rename(\n",
    "    columns={\"1stFlrSF\": \"FirstFlrSF\", \"2ndFlrSF\": \"SecondFlrSF\", \"3SsnPorch\": \"Threeseasonporch\"},\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df_train.select_dtypes(exclude=[\"object\"])\n",
    "df_cat = df_train.select_dtypes(include=[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коррелирующие признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 5, figsize=(20, 20))\n",
    "axes_flattened = axes.reshape(-1)\n",
    "for i in range(len(df_num.columns)):\n",
    "    ax = axes_flattened[i]\n",
    "    sns.scatterplot(\n",
    "        x=df_num.iloc[:, i],\n",
    "        y=\"SalePrice\",\n",
    "        data=df_num.dropna(),\n",
    "        ax=ax,\n",
    "    )\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_plot(df: pd.DataFrame, method: str = \"pearson\", annot: bool = True, **kwargs) -> None:\n",
    "    sns.clustermap(\n",
    "        df.corr(method),\n",
    "        vmin=-1.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"icefire\",\n",
    "        method=\"complete\",\n",
    "        annot=annot,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "corr_plot(df_num, annot=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этой матрицы можно увидеть, какие столбцы сильно коррелируют между собой, например:\n",
    "1. GarageYrBlt и YearBuilt\n",
    "2. TotRmsAbvGrd и GrLivArea\n",
    "3. FirstFlrSF и TotalBsmtSF\n",
    "4. GarageArea и GarageCars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(\n",
    "    [\"GarageYrBlt\", \"TotRmsAbvGrd\", \"FirstFlrSF\", \"GarageCars\"],\n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполнение пустых значений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может быть множество вариантов, при которых строка может содержать пустые значения. Например:\n",
    "1. Дом с 2 спальнями не может включать ответ на вопрос, насколько велика третья спальня\n",
    "2. Кто-то из опрошенных может не делиться своим доходом\n",
    "\n",
    "Библиотеки Python представляют недостающие числа как NaN-ы, что является сокращением от \"not a number\".\n",
    "\n",
    "Соберем статистику, связанную с NaN-ми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan = (df_train.isnull().mean() * 100).reset_index()\n",
    "df_nan.columns = [\"column_name\", \"percentage\"]\n",
    "df_nan.sort_values(\"percentage\", ascending=False, inplace=True)\n",
    "df_nan.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем квантили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percent in (80, 50, 20, 5):\n",
    "    print(f\"Number of columns with more than {percent}% NANs: {(df_nan.percentage > percent).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем столбцы с более чем 80% NaN-в"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_columns = list(df_nan[df_nan.percentage > 80][\"column_name\"])\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство моделей не умеют работать с NaN-ми. Поэтому требуется избавиться от них.\n",
    "\n",
    "### Выброс столбцов с NaN-ми"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 вариант - если, например, нужно выбросить одинаковые столбцы для обучающей и тестовой выборок\n",
    "num_сols_with_missing = [\n",
    "    col for col in df_num.columns if df_num[col].isnull().any()\n",
    "]\n",
    "num_сols_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_num.columns))\n",
    "df_num_dropped = df_num.drop(num_сols_with_missing, axis=1)\n",
    "print(len(df_num_dropped.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 вариант: выбросить столбцы, напрямую используя `dropna()`\n",
    "print(len(df_num.columns))\n",
    "df_num_dropped = df_num.dropna(axis=1)\n",
    "print(len(df_num_dropped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если эти столбцы содержат полезную информацию (в местах, которые не были пропущены), модель теряет доступ к этой информации при удалении столбца. Кроме того, если тестовые данные имеют отсутствующие значения в тех местах, где тренировочные не имели, это приведет к ошибке.\n",
    "\n",
    "Так что обычно это не лучшее решение. Однако оно может быть полезно, когда большинство значений в столбце отсутствуют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение недостающих значений каким-то значением\n",
    "\n",
    "Это значение будет не совсем правильным в большинстве случаев, но обычно оно дает более точные модели, чем полное удаление столбца."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Числовые признаки**\n",
    "\n",
    "Поведение по умолчанию заполняет столбец средним значением в заполненных ячейках. Существуют и более сложные стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_imputer = SimpleImputer()\n",
    "\n",
    "filled_cols = my_imputer.fit_transform(df_train[num_сols_with_missing])\n",
    "filled_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативно можно заполнить столбцы средним напрямую (или нулями, или чем угодно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[num_сols_with_missing].fillna(df_train[num_сols_with_missing].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения статистики такое заполнение оправдано, если между признаками нет явной зависимости. В таком случае замена пропусков средними значениями не вносит смещения. Однако, часто условие независимости нарушается. В данном примере свойства домов сильно зависят от того, в каком районе они расположены. Поэтому средние значения лучше считать по районам.\n",
    "\n",
    "Взглянем на распределения средних значений по районам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_grouped = df_train.groupby(\"Neighborhood\")\n",
    "neigh_lot = (\n",
    "    neigh_grouped[\"LotFrontage\"].mean()\n",
    "    .reset_index(name=\"LotFrontage_mean\")\n",
    ")\n",
    "neigh_garage = (\n",
    "    neigh_grouped[\"GarageArea\"].mean()\n",
    "    .reset_index(name=\"GarageArea_mean\")\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(22,8))\n",
    "axes[0].tick_params(axis=\"x\", rotation=90)\n",
    "sns.barplot(x=\"Neighborhood\", y=\"LotFrontage_mean\", data=neigh_lot, ax=axes[0])\n",
    "axes[1].tick_params(axis=\"x\", rotation=90)\n",
    "sns.barplot(x=\"Neighborhood\", y=\"GarageArea_mean\", data=neigh_garage, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.mean()))\n",
    "df_train[\"GarageArea\"] = df_train.groupby(\"Neighborhood\")[\"GarageArea\"].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним все оставшиеся числовые признаки средними (ранее мы не сохраняли результат в `df_train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[num_сols_with_missing] = df_train[num_сols_with_missing].fillna(df_train[num_сols_with_missing].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Те столбцы, которые содержали более 80% NANов, удалим совсем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(nan_columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Категориальные (номинальные) признаки**\n",
    "\n",
    "Понятие среднего здесь тяжело использовать, поэтому проще заполнить модой, то есть наиболее часто встречающимся значением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"]\n",
    "for col in cols:\n",
    "    print(f\"Mode of column {col} is {df_train[col].dropna().mode()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[cols] = df_train.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.dropna().mode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Порядковые признаки**\n",
    "\n",
    "Мы можем их заполнить средним или часто встречающимся, но также можно использовать значение по умолчанию \"NA\". Это значение будет удобно ассоциировать с нулем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [\n",
    "    \"GarageType\", \"GarageFinish\", \"BsmtFinType2\", \"BsmtExposure\", \"BsmtFinType1\",\n",
    "    \"GarageCond\", \"GarageQual\", \"BsmtCond\", \"BsmtQual\", \"FireplaceQu\", \"KitchenQual\",\n",
    "    \"HeatingQC\", \"ExterQual\", \"ExterCond\"\n",
    "]\n",
    "df_train[cat] = df_train[cat].fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление признаков со слабой вариативностью\n",
    "\n",
    "**Признаки с одним типичным значением**\n",
    "\n",
    "Некоторые признаки в основном состоят из одного значения или нулей, что не особо полезно для нас. Поэтому мы устанавливаем пороговое значение, определяемое пользователем, на уровне 95%. Если столбец имеет более 95% от одного и того же значения, мы считаем признак бесполезными и удалим его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_almost_constant_columns(df: pd.DataFrame, dropna: bool = True) -> list[str]:\n",
    "    cols = []\n",
    "    for i in df:\n",
    "        counts = df[i].dropna().value_counts() if dropna else df[i].value_counts()\n",
    "        most_popular_value_count = counts.iloc[0]\n",
    "        if (most_popular_value_count / len(df)) * 100 > 95:\n",
    "            cols.append(i)\n",
    "\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_train.select_dtypes(include=[\"object\"])\n",
    "overfit_cat = get_almost_constant_columns(df_cat)\n",
    "df_train = df_train.drop(overfit_cat, axis=1)\n",
    "overfit_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = df_train.select_dtypes(exclude=[\"object\"])\n",
    "overfit_num = get_almost_constant_columns(df_num, dropna=True)\n",
    "df_train = df_train.drop(overfit_num, axis=1)\n",
    "overfit_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Признаки с маленькой дисперсией**\n",
    "\n",
    "Другой способ - использовать метод VarianceThreshold от sklearn — это простой базовый подход к выбору признаков. Он удаляет все признаки, дисперсия которых не соответствует определенному порогу. По умолчанию он удаляет все элементы с нулевой дисперсией, т.е. те элементы, которые имеют одинаковое значение у всех семплов.\n",
    "\n",
    "Стоит отметить, что дисперсия является абсолютной величиной, и выбор порога в этом случае является эмпирическим. При этом в общем случае малые значения дисперсии не говорят о бесполезности признака. Если признак задан на поле вещественных чисел, то его дискриминирующая способность не зависит от дисперсии, так как любой непрерывный интервал на вещественной оси содержит бесконечный набор значений. Однако, в случае дискретных значений (пример, целочисленных признаков) VarianceThreshold действительно становится полезным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "fs = VarianceThreshold(threshold=0.1)\n",
    "num_col = df_train.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "fs.fit(num_col)  # fit finds the features with low variance\n",
    "sum(fs.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `get_support()` возвратит булевскую маску для признаков, которые проходят указанный порог по дисперсии. Ее можно использовать для отбора этих признаков "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, таким образом мы получаем список всех признаков, которые были отсеяны данным алгоритмом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col.columns[~fs.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление выбросов\n",
    "\n",
    "Удаление выбросов предотвратит воздействие экстремальных значений на производительность наших моделей.\n",
    "\n",
    "Из скаттерплотов выше мы можем увидеть, что следующие признаки имеют экстремальные выбросы:\n",
    "\n",
    "* LotFrontage\n",
    "* LotArea\n",
    "* BsmtFinSF1\n",
    "* TotalBsmtSF\n",
    "* GrLivArea\n",
    "\n",
    "Мы уберем выбросы на основе определенного порогового значения.\n",
    "Эти значения мы получим из боксплотов (\"ящик с усиками\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_col = [\"LotFrontage\", \"LotArea\", \"BsmtFinSF1\", \"TotalBsmtSF\", \"GrLivArea\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for ax, col in zip(axes, out_col):\n",
    "    sns.boxplot(y=df_train[col], data=df_train, ax=ax)\n",
    "    \n",
    "fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, upper_bound in (\n",
    "    (\"LotFrontage\", 200),\n",
    "    (\"LotArea\", 100000),\n",
    "    (\"BsmtFinSF1\", 4000),\n",
    "    (\"TotalBsmtSF\", 5000),\n",
    "    (\"GrLivArea\", 4000),\n",
    "):\n",
    "    df_train = df_train.drop(df_train[df_train[col] > upper_bound].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После удаления выбросов, сильно коррелированных признаков и условных отсутствующих значений мы можем приступить к добавлению дополнительной информации для обучения нашей модели. Это делается с помощью - Feature Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - это техника, с помощью которой мы создаем новые признаки, которые потенциально могут помочь в прогнозировании нашей целевой переменной, которая в данном случае является SalePrice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSSubClass - это столбец с числовым признаком, который на самом деле можно представить как категориальный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"MSSubClass\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"MSSubClass\"] = df_train[\"MSSubClass\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_map = {\"Ex\": 5,\"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0}\n",
    "fintype_map = {\"GLQ\": 6,\"ALQ\": 5,\"BLQ\": 4,\"Rec\": 3,\"LwQ\": 2,\"Unf\": 1, \"NA\": 0}\n",
    "expose_map = {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"NA\": 0}\n",
    "# fence_map = {\"GdPrv\": 4,\"MnPrv\": 3,\"GdWo\": 2, \"MnWw\": 1,\"NA\": 0}  -- выброшен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_col = [\n",
    "    \"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \n",
    "    \"KitchenQual\", \"GarageQual\", \"GarageCond\", \"FireplaceQu\"\n",
    "]\n",
    "for col in ord_col:\n",
    "    df_train[col] = df_train[col].map(ordinal_map)\n",
    "    \n",
    "fin_col = [\"BsmtFinType1\", \"BsmtFinType2\"]\n",
    "for col in fin_col:\n",
    "    df_train[col] = df_train[col].map(fintype_map)\n",
    "\n",
    "df_train[\"BsmtExposure\"] = df_train[\"BsmtExposure\"].map(expose_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Основываясь на текущих признаках, мы можем добавить первый дополнительный признак, который будет называться TotalLot и который суммирует LotFrontage и LotArea для определения общей площади земли, доступной в виде лота.\n",
    "\n",
    "  TotalLot = LotFrontage + LotArea\n",
    "\n",
    "- Мы также можем рассчитать общее количество площади поверхности дома, TotalSF, сложив площадь от 1-го этажа и 2-го этажа.\n",
    "  \n",
    "  TotalSF = TotalBsmtSF + 2ndFlrSF\n",
    "  \n",
    "- TotalBath также может быть использован, чтобы сказать нам в общей сложности, сколько ванных комнат есть в доме.\n",
    "\n",
    "  TotalBath = FullBath + HalfBath\n",
    "  \n",
    "- Мы также можем добавить все различные типы крылец вокруг дома и обобщить в общей площади крыльца, TotalPorch.\n",
    "\n",
    "  TotalPorch = OpenPorchSF + EnclosedPorch + ScreenPorch\n",
    "\n",
    "- TotalBsmtFin = BsmtFinSF1 + BsmtFinSF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"TotalLot\"] = df_train[\"LotFrontage\"] + df_train[\"LotArea\"]\n",
    "df_train[\"TotalBsmtFin\"] = df_train[\"BsmtFinSF1\"] + df_train[\"BsmtFinSF2\"]\n",
    "df_train[\"TotalSF\"] = df_train[\"TotalBsmtSF\"] + df_train[\"SecondFlrSF\"]\n",
    "df_train[\"TotalBath\"] = df_train[\"FullBath\"] + df_train[\"HalfBath\"]\n",
    "df_train[\"TotalPorch\"] = df_train[\"OpenPorchSF\"] + df_train[\"EnclosedPorch\"] + df_train[\"ScreenPorch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"LivLotRatio\"] = df_train[\"GrLivArea\"] / df_train[\"LotArea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также включаем создание бинарных столбцов для некоторых признаков, которые могут указывать на наличие(1) / отсутствие(0) некоторых признаков дома"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"MasVnrArea\", \"TotalBsmtFin\", \"TotalBsmtSF\", \"SecondFlrSF\", \"WoodDeckSF\", \"TotalPorch\"\n",
    "]\n",
    "for col in cols:\n",
    "    col_name = col + \"_bin\"\n",
    "    df_train[col_name] = df_train[col].apply(lambda df_train: 1 if df_train > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем наборе достаточно много категориальных признаков, и использовать их в таком виде, как они представлены в датасете, скорее всего нельзя. Это связано с тем, что модели в большинстве своем работают с евклидовыми или метрическими пространствами. Для перевода категориальных признаков в них используются различные техники, рассмотрим некоторые из них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Encoding**\n",
    "\n",
    "Естественным решением такой проблемы было бы однозначное отображение каждого значения в уникальное число. К примеру, мы могли бы преобразовать признак Street так: Pave в 0, а Grvl в 1. Эту простую операцию приходится делать часто, поэтому в модуле sklearn.preprocessing  именно для этой задачи реализован класс LabelEncoder. \n",
    "\n",
    "Метод fit этого класса находит все уникальные значения и строит таблицу для соответствия каждой категории некоторому числу, а метод transform непосредственно преобразует значения в числа. После fit у label_encoder будет доступно поле classes_, содержащее все уникальные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"LandContour\"].value_counts().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "encoded_neigh = pd.Series(label_encoder.fit_transform(df_train[\"LandContour\"]))\n",
    "sns.histplot(encoded_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(22,8))\n",
    "axes[0].tick_params(axis=\"x\", rotation=90)\n",
    "sns.histplot(df_train[\"LandContour\"], ax=axes[0])\n",
    "axes[1].tick_params(axis=\"x\", rotation=90)\n",
    "sns.histplot(encoded_neigh, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что произойдет, если у нас появятся данные с другими категориями? LabelEncoder выдаст ошибку, что в словаре нет такой категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.transform(df_train[\"LandContour\"].replace(\"Low\", \"low\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, при использовании этого метода нужно быть уверенным, что признак не может принимать неизвестных ранее значений. \n",
    "\n",
    "Основная проблема такого представления заключается даже не в этом, а в том, что числовой код создал евклидово представление для данных. Это значит, что теперь можно вычесть \"Low\" из \"Bnk\" и т.д. Поэтому, например, методы, основанные на расстоянии, становятся больше неприменимы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Hot encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot encoding является наиболее распространенным подходом для преобразования категориальных признаков, и он работает очень хорошо, если ваша категориальная переменная принимает небольшое количество значений (т.е. вы, как правило, не будете этого делать для переменных, которые принимают более 15 различных значений)\n",
    "\n",
    "Предположим, что некоторый признак может принимать 10 разных значений. В этом случае One Hot Encoding подразумевает создание 10 признаков, все из которых равны нулю за исключением одного. На позицию, соответствующую численному значению признака мы помещаем 1.\n",
    "Этот метод реализован в sklearn.preprocessing в классе OneHotEncoder. По умолчанию OneHotEncoder преобразует данные в разреженную матрицу, чтобы не расходовать память на хранение многочисленных нулей. Однако в нашем случае размер данных не является проблемой, поэтому мы будем использовать \"плотное\" представление.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "encoded_categorical_columns = pd.DataFrame(onehot_encoder.fit_transform(df_cat))\n",
    "encoded_categorical_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, можно сразу удалить категории, которые встречаются редко. Это можно сделать, задав значение параметра min_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse_output=False, min_frequency=0.3)\n",
    "encoded_categorical_columns = pd.DataFrame(onehot_encoder.fit_transform(df_cat))\n",
    "encoded_categorical_columns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для категориальных столбцов в pandas можно применить one-hot-encoding с помощью метода get_dummies()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.get_dummies(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot-encoding-ом и label encoding-ом выбор не ограничен. Существует достаточно много альтернативных вариантов преобразования категориальных переменных. Если тема заинтересовала, обратите внимание на библиотеку [category-encoders](https://pypi.org/project/category-encoders/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация/Скейлинг\n",
    "RobustScaler - это метод преобразования, который удаляет медиану и масштабирует данные в соответствии с диапазоном квантиля (по умолчанию IQR: межквартильный диапазон). IQR - это диапазон между 1-м квартилем (25-й квантилем) и 3 Квартиль (75-й квантиль). Он также устойчив к выпадающим значениям, что делает его идеальным для данных, где слишком много выпадающих значений, что резко сократит количество обучающих данных.\n",
    "\n",
    "Запуская скейлер как на тренировочном, так и на тестовом наборах, мы подвергаем себя проблеме утечки данных. Утечка данных - это проблема, когда для создания модели используется информация извне набора для обучения. Если мы подгоняем скейлер как на тренировочные, так и на тестовые данные, наши характеристики тренировочных данных будут содержать распределение нашего тестового набора. Таким образом, мы неявно передаем информацию о наших тестовых данных в окончательные тренировочные данные для обучения, что не даст нам возможности по-настоящему протестировать нашу модель на данных, которые она никогда не видела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_train.select_dtypes(np.number).columns\n",
    "# df_train = df_train.drop([\"Id\"], axis=1)\n",
    "transformer = RobustScaler().fit(df_train[cols])\n",
    "df_train[cols] = transformer.transform(df_train[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы сильно преобразовали наш обучающий набор. кроме перчисленного, полезно использовать PCA, выбор признаков на основе информации и други методы. Как вы, наверное, заметили, все преобразования были сделаны только для тренировочного набора, но то же самое необходимо сделать и для тестового.\n",
    "\n",
    "Чтобы предотвратить утечку данных, все преобразования по среднему и т.п. нужно сделать независимо, а если мы, например, кодировали или удаляли столбцы, нужно сделать такое же преобразование, используя старые правила. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b3714695f2307aafe7da52bf6e53e38bc5469a267534973be7d21c816457eaf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
