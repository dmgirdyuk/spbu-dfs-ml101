{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика по линейной регрессии и методу главных компонент\n",
    "\n",
    "Датасет скачивать по ссылке: https://disk.yandex.ru/d/cwL3Ka4ECyQwpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\n",
    "\n",
    "SEED = 314159\n",
    "TRAIN_TEST_SPLIT = 0.75\n",
    "\n",
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = pd.read_csv(cfg[\"house_prices\"][\"train_dataset\"])\n",
    "y = df_init[\"SalePrice\"]\n",
    "df_init.drop(columns=[\"SalePrice\", \"Id\"], inplace=True)\n",
    "\n",
    "df_train, df_test, y_train, y_test = train_test_split(df_init, y, test_size = 1 - TRAIN_TEST_SPLIT, random_state=SEED)\n",
    "\n",
    "dfs = (df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n",
    "    \n",
    "    # names beginning with numbers are awkward to work with\n",
    "    df.rename(\n",
    "        columns={\"1stFlrSF\": \"FirstFlrSF\", \"2ndFlrSF\": \"SecondFlrSF\", \"3SsnPorch\": \"Threeseasonporch\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    \n",
    "    # удаляем скореллированные признаки\n",
    "    df.drop(\n",
    "        columns=[\"GarageYrBlt\", \"TotRmsAbvGrd\", \"FirstFlrSF\", \"GarageCars\"],\n",
    "        inplace=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение пустых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not y_train.isna().sum() and not y_test.isna().sum()\n",
    "assert not df_train[\"Neighborhood\"].isna().sum() and not df_test[\"Neighborhood\"].isna().sum()\n",
    "\n",
    "# выбрасываем признаки, процент пропущенных значений у которых больше 80\n",
    "nan_info_train = (df_train.isnull().mean() * 100).reset_index()\n",
    "nan_info_train.columns = [\"column_name\", \"percentage\"]\n",
    "nan80_cols = list(nan_info_train[nan_info_train.percentage > 80][\"column_name\"])\n",
    "\n",
    "print(\"Отбрасываем\", nan80_cols)\n",
    "for df in dfs:\n",
    "    df.drop(columns=nan80_cols, inplace=True)\n",
    "\n",
    "print(\"Суммарной количество пропущенных значений\", df_train.isna().sum().sum(), df_test.isna().sum().sum())\n",
    "\n",
    "# создаем новую категорию для подмножества категориальных признаков\n",
    "na_cat_cols = [\n",
    "    \"GarageType\", \"GarageFinish\", \"BsmtFinType2\", \"BsmtExposure\", \"BsmtFinType1\",\n",
    "    \"GarageCond\", \"GarageQual\", \"BsmtCond\", \"BsmtQual\", \"FireplaceQu\", \"KitchenQual\",\n",
    "    \"HeatingQC\", \"ExterQual\", \"ExterCond\"\n",
    "]\n",
    "for df in dfs:\n",
    "    df[na_cat_cols] = df[na_cat_cols].fillna(\"NA\")\n",
    "\n",
    "print(\"Суммарной количество пропущенных значений\", df_train.isna().sum().sum(), df_test.isna().sum().sum())\n",
    "\n",
    "# заполняем медианой с учетом группы\n",
    "na_group_fill_num_cols = [\"LotFrontage\", \"GarageArea\"]\n",
    "for col in na_group_fill_num_cols:\n",
    "    na_group_fill_num_mapping = df_train.groupby(\"Neighborhood\")[col].median().to_dict()\n",
    "    for df in dfs:\n",
    "        for neighb, fill_value in na_group_fill_num_mapping.items():\n",
    "            mask = df[\"Neighborhood\"] == neighb\n",
    "            df.loc[mask, col] = df.loc[mask, col].fillna(fill_value)\n",
    "            \n",
    "print(\"Суммарной количество пропущенных значений\", df_train.isna().sum().sum(), df_test.isna().sum().sum())\n",
    "\n",
    "# заполняем модой с учетом группы\n",
    "na_group_fill_cat_cols = [\n",
    "    \"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"\n",
    "]\n",
    "for col in na_group_fill_cat_cols:\n",
    "    fill_value_gen = df_train[col].dropna().mode()[0]\n",
    "    for neighb, df_group in df_train.groupby(\"Neighborhood\"):\n",
    "        mode_output = df_group[col].dropna().mode()\n",
    "        fill_value = mode_output[0] if len(mode_output) else fill_value_gen\n",
    "        for df in dfs:\n",
    "            mask = df[\"Neighborhood\"] == neighb\n",
    "            df.loc[mask, col] = df.loc[mask, col].fillna(fill_value)\n",
    "\n",
    "print(\"Суммарной количество пропущенных значений\", df_train.isna().sum().sum(), df_test.isna().sum().sum())\n",
    "\n",
    "# заполняем медианой и модой все столбцы с пропущенными значениями\n",
    "num_cols = df_train.select_dtypes(exclude=[\"object\"]).columns\n",
    "cat_cols = df_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy=\"median\").fit(df_train[num_cols])\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\").fit(df_train[cat_cols])\n",
    "for df in dfs:\n",
    "    df[num_cols] = num_imputer.transform(df[num_cols])\n",
    "    df[cat_cols] = cat_imputer.transform(df[cat_cols])\n",
    "\n",
    "print(\"Суммарной количество пропущенных значений\", df_train.isna().sum().sum(), df_test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отбрасываем \"скучные\" признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_almost_constant_columns(df: pd.DataFrame, dropna: bool = True) -> list[str]:\n",
    "    cols = []\n",
    "    for i in df:\n",
    "        counts = df[i].dropna().value_counts() if dropna else df[i].value_counts()\n",
    "        most_popular_value_count = counts.iloc[0]\n",
    "        if (most_popular_value_count / len(df)) * 100 > 95:\n",
    "            cols.append(i)\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "# отбрасываем категориальные столбцы, у которых 95+% повторяющихся значений\n",
    "almost_constant_cat_cols = get_almost_constant_columns(df_train.select_dtypes(include=[\"object\"]))\n",
    "print(\"Отбрасываем\", almost_constant_cat_cols)\n",
    "for df in dfs:\n",
    "    df.drop(columns=almost_constant_cat_cols, inplace=True)\n",
    "\n",
    "\n",
    "# удаляем столбцы в интервальной шкале с незначительной дисперсией\n",
    "df_train_num = df_train.select_dtypes(exclude=[\"object\"])\n",
    "var_thd = VarianceThreshold(threshold=0.1).fit(df_train_num)\n",
    "low_var_cols = df_train_num.columns[~var_thd.get_support()]\n",
    "print(\"Отбрасываем\", low_var_cols)\n",
    "for df in dfs:\n",
    "    df.drop(columns=low_var_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаляем выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, upper_bound in (\n",
    "    (\"LotFrontage\", 200),\n",
    "    (\"LotArea\", 100000),\n",
    "    (\"BsmtFinSF1\", 4000),\n",
    "    (\"TotalBsmtSF\", 5000),\n",
    "    (\"GrLivArea\", 4000),\n",
    "):\n",
    "    drop_index = df_train[df_train[col] > upper_bound].index\n",
    "    df_train = df_train.drop(drop_index, axis=0)\n",
    "    y_train = y_train.drop(drop_index, axis=0)\n",
    "\n",
    "    drop_index = df_test[df_test[col] > upper_bound].index\n",
    "    df_test = df_test.drop(drop_index, axis=0)\n",
    "    y_test = y_test.drop(drop_index, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_map = {\"Ex\": 5,\"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0}\n",
    "fintype_map = {\"GLQ\": 6,\"ALQ\": 5,\"BLQ\": 4,\"Rec\": 3,\"LwQ\": 2,\"Unf\": 1, \"NA\": 0}\n",
    "expose_map = {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"NA\": 0}\n",
    "# fence_map = {\"GdPrv\": 4,\"MnPrv\": 3,\"GdWo\": 2, \"MnWw\": 1,\"NA\": 0}  -- выброшен\n",
    "\n",
    "ord_col = [\n",
    "    \"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \n",
    "    \"KitchenQual\", \"GarageQual\", \"GarageCond\", \"FireplaceQu\"\n",
    "]\n",
    "fin_col = [\"BsmtFinType1\", \"BsmtFinType2\"]\n",
    "\n",
    "for df in dfs:\n",
    "    # столбец с числовым признаком, который на самом деле можно представить как категориальный\n",
    "    df[\"MSSubClass\"] = df[\"MSSubClass\"].apply(str)\n",
    "\n",
    "    for col in ord_col:\n",
    "        df[col] = df[col].map(ordinal_map)\n",
    "\n",
    "    for col in fin_col:\n",
    "        df[col] = df[col].map(fintype_map)\n",
    "\n",
    "    df[\"BsmtExposure\"] = df[\"BsmtExposure\"].map(expose_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed = df_train.copy()\n",
    "df_test_processed = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_processed.copy()\n",
    "df_test = df_test_processed.copy()\n",
    "\n",
    "ADD_CUSTOM_FEATURES = True\n",
    "APPLY_PCA = True\n",
    "ADD_OHE_CAT = True\n",
    "PCA_COMPONENTS_NUM = 5\n",
    "NORMALIZE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_bin = [\n",
    "    \"MasVnrArea\", \"TotalBsmtFin\", \"TotalBsmtSF\", \"SecondFlrSF\", \"WoodDeckSF\", \"TotalPorch\"\n",
    "]\n",
    "\n",
    "\n",
    "def add_new_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"TotalLot\"] = df[\"LotFrontage\"] + df[\"LotArea\"]\n",
    "    df[\"TotalBsmtFin\"] = df[\"BsmtFinSF1\"] + df[\"BsmtFinSF2\"]\n",
    "    df[\"TotalSF\"] = df[\"TotalBsmtSF\"] + df[\"SecondFlrSF\"]\n",
    "    df[\"TotalBath\"] = df[\"FullBath\"] + df[\"HalfBath\"]\n",
    "    df[\"TotalPorch\"] = df[\"OpenPorchSF\"] + df[\"EnclosedPorch\"] + df[\"ScreenPorch\"]\n",
    "    df[\"LivLotRatio\"] = df[\"GrLivArea\"] / df[\"LotArea\"]\n",
    "\n",
    "    for col in cols_to_bin:\n",
    "        df.loc[:, f\"{col}_bin\"] = df[col].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# добавим новых признаков\n",
    "if ADD_CUSTOM_FEATURES:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        df_train = add_new_features(df_train)\n",
    "        df_test = add_new_features(df_test)\n",
    "\n",
    "num_cols = df_train.select_dtypes(exclude=[\"object\"]).columns\n",
    "cat_cols = df_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# метод главных компонент\n",
    "if APPLY_PCA:\n",
    "    print(\"Количество признаков в интервальной шкале\", len(num_cols))\n",
    "    pca = PCA(n_components=PCA_COMPONENTS_NUM, random_state=SEED)\n",
    "    \n",
    "    df_train_pca = pd.DataFrame(\n",
    "        pca.fit_transform(df_train[num_cols]),\n",
    "        columns=pca.get_feature_names_out(),\n",
    "        index=df_train.index,\n",
    "    )\n",
    "    df_test_pca = pd.DataFrame(\n",
    "        pca.transform(df_test[num_cols]),\n",
    "        columns=pca.get_feature_names_out(),\n",
    "        index=df_test.index,\n",
    "    )\n",
    "    \n",
    "    df_train = pd.concat([df_train_pca, df_train[cat_cols]], axis=1)\n",
    "    df_test = pd.concat([df_test_pca, df_test[cat_cols]], axis=1)\n",
    "    num_cols = df_train.select_dtypes(exclude=[\"object\"]).columns\n",
    "\n",
    "\n",
    "# one hot encoding\n",
    "if ADD_OHE_CAT:\n",
    "    onehot_encoder = OneHotEncoder(\n",
    "        sparse_output=False, min_frequency=0.3, handle_unknown=\"ignore\"\n",
    "    ).fit(df_train[cat_cols])\n",
    "    df_train_ohe = pd.DataFrame(\n",
    "        onehot_encoder.transform(df_train[cat_cols]),\n",
    "        columns=onehot_encoder.get_feature_names_out(),\n",
    "        index=df_train.index,\n",
    "    )\n",
    "    df_test_ohe = pd.DataFrame(\n",
    "        onehot_encoder.transform(df_test[cat_cols]),\n",
    "        columns=onehot_encoder.get_feature_names_out(),\n",
    "        index=df_test.index,\n",
    "    )\n",
    "    df_train = pd.concat([df_train[num_cols], df_train_ohe], axis=1)\n",
    "    df_test = pd.concat([df_test[num_cols], df_test_ohe], axis=1)\n",
    "else:\n",
    "    df_train = df_train[num_cols]\n",
    "    df_test = df_test[num_cols]\n",
    "\n",
    "# нормализация\n",
    "if NORMALIZE or APPLY_PCA:\n",
    "    num_cols = df_train.select_dtypes(np.number).columns\n",
    "    scaler = RobustScaler()\n",
    "    df_train[num_cols] = scaler.fit_transform(df_train[num_cols])\n",
    "    df_test[num_cols] = scaler.transform(df_test[num_cols])\n",
    "    # df_train[df_train.columns] = scaler.fit_transform(df_train)\n",
    "    # df_test[df_test.columns] = scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "ridge_001 = Ridge(alpha=0.01, random_state=SEED)\n",
    "ridge_01 = Ridge(alpha=0.1, random_state=SEED)\n",
    "ridge_1 = Ridge(alpha=1.0, random_state=SEED)\n",
    "lasso_001 = Lasso(alpha=0.01, random_state=SEED)\n",
    "lasso_01 = Lasso(alpha=0.01, random_state=SEED)\n",
    "lasso_1 = Lasso(alpha=1.0, random_state=SEED)\n",
    "elastic_net_001_001 = ElasticNet(alpha=0.01, l1_ratio=0.01, random_state=SEED)\n",
    "elastic_net_01_01 = ElasticNet(alpha=0.1, l1_ratio=0.1, random_state=SEED)\n",
    "elastic_net_1_1 = ElasticNet(alpha=1.0, l1_ratio=1.0, random_state=SEED)\n",
    "\n",
    "models = dict(\n",
    "    zip(\n",
    "        [\n",
    "            \"linreg\", \"ridge_001\", \"ridge_01\", \"ridge_1\", \"lasso_001\", \"lasso_01\", \"lasso_1\",\n",
    "            \"elastic_net_001_001\", \"elastic_net_01_01\", \"elastic_net_1_1\"\n",
    "        ],\n",
    "        [\n",
    "            linreg, ridge_001, ridge_01, ridge_1, lasso_001, lasso_01, lasso_1, \n",
    "            elastic_net_001_001, elastic_net_01_01, elastic_net_1_1\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for model in models.values():\n",
    "        model.fit(df_train, y_train)\n",
    "\n",
    "y_preds = {}\n",
    "for model_name, model in models.items():\n",
    "    y_preds[model_name] = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for model_name, y_pred in y_preds.items():\n",
    "    data.append(\n",
    "        [\n",
    "            model_name,\n",
    "            mean_absolute_error(y_true=y_test, y_pred=y_pred),\n",
    "            np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred)),\n",
    "            r2_score(y_true=y_test, y_pred=y_pred),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df_res = pd.DataFrame(data, columns=[\"model_name\", \"MAE\", \"RMSE\", \"R2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(pd.DataFrame({\"true\": y_test, \"pred\": y_preds[\"elastic_net_01_01\"]}), x=\"pred\", y=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    pd.DataFrame({\"index\": range(len(y_test)), \"diff\": y_test - y_preds[\"elastic_net_01_01\"]}),\n",
    "    x=\"index\",\n",
    "    y=\"diff\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b3714695f2307aafe7da52bf6e53e38bc5469a267534973be7d21c816457eaf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
